from TensorFlow.keras import initializers
from TensorFlow.keras import activations
from TensorFlow.keras import backend as K
from TensorFlow.keras.engine.topology import Layer
 
class MyMultiHeadAttention(Layer):
    def __init__(self,output_dim,num_head,kernel_initializer='glorot_uniform',**kwargs):
        self.output_dim=output_dim
        self.num_head=num_head
        self.kernel_initializer = initializers.get(kernel_initializer)
        super(MyMultiHeadAttention,self).__init__(**kwargs)
        
    def build(self,input_shape):
        self.W=self.add_weight(name='W',
           shape=(self.num_head,3,input_shape[2],self.output_dim),
           initializer=self.kernel_initializer,
           trainable=True)
        self.Wo=self.add_weight(name='Wo',
           shape=(self.num_head*self.output_dim,self.output_dim),
           initializer=self.kernel_initializer,
           trainable=True)
        self.built = True
        
    def call(self,x):
        q=K.dot(x,self.W[0,0])
        k=K.dot(x,self.W[0,1])
        v=K.dot(x,self.W[0,2])
        e=K.batch_dot(q,K.permute_dimensions(k,[0,2,1]))#  kת ã     q   
        e=e/(self.output_dim**0.5)
        e=K.softmax(e)
        outputs=K.batch_dot(e,v)
        for i in range(1,self.W.shape[0]):
            q=K.dot(x,self.W[i,0])
            k=K.dot(x,self.W[i,1])
            v=K.dot(x,self.W[i,2])
            #print('q_shape:'+str(q.shape))
            e=K.batch_dot(q,K.permute_dimensions(k,[0,2,1]))#  kת ã     q   
            e=e/(self.output_dim**0.5)
            e=K.softmax(e)
            #print('e_shape:'+str(e.shape))
            o=K.batch_dot(e,v)
            outputs=K.concatenate([outputs,o])
        z=K.dot(outputs,self.Wo)
        return z
        
    def compute_output_shape(self,input_shape):
        return (input_shape[0],input_shape[1],self.output_dim)

        typeError: __init__() got an unexpected keyword argument 'name'